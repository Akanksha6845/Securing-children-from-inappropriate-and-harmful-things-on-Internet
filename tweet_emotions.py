# -*- coding: utf-8 -*-
"""tweet_emotions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YewdCvD7FAonRRgLifAf3cVxlzKZvIyd
"""

import pandas as pd

from google.colab import files
uploaded = files.upload()  # A file selection dialog will appear

data=pd.read_csv('tweet_emotions.csv')

data.head(10)

"""Removing punctuations and symbol"""

import pandas as pd

# Define the set of punctuations to remove
punctuations = '''!()-[]{};:'"\,<>./?@#$%^&*_~'''

# Input and output file paths
input_csv = 'tweet_emotions.csv'   # Replace with your input CSV file
output_csv = 'cleaned_tweet_emotions.csv'  # Replace with your desired output CSV file

# Load the dataset
df = pd.read_csv(input_csv)

# Check if the column exists (assuming the column is named 'content')
if 'content' not in df.columns:
    raise ValueError("The CSV file must have a 'content' column with text data to clean.")

# Function to remove punctuation
def remove_punctuation(text):
    return ''.join(char for char in str(text) if char not in punctuations)

# Apply the function to the 'content' column
df['Cleaned_Content'] = df['content'].apply(remove_punctuation)

# Save the updated DataFrame to a new CSV file
df.to_csv(output_csv, index=False)

print(f"Cleaned dataset saved to {output_csv}")

df

import re
s="string .with. Punctuation?"
s=re.sub(r'[^\w\s]','',s)
print(s)

"""Tokenization"""

import nltk
nltk.download('punkt_tab')
nltk.download('wordnet')

nltk.word_tokenize("hi! how are you?")

# Input and output file paths
input_csv = 'cleaned_tweet_emotions.csv'   # Replace with your input CSV file
output_csv = 'tokenized_cleaned_tweet_emotions.csv'  # Replace with your desired output CSV file

# Load the dataset
df = pd.read_csv(input_csv)

# Check if 'Cleaned_Content' column exists
if 'Cleaned_Content' not in df.columns:
    raise ValueError("The CSV file must have a 'content' column with text data to tokenize.")

# Import word_tokenize
from nltk.tokenize import word_tokenize # Importing the necessary function

# Tokenize the 'content' column
df['Tokenized_Content'] = df['Cleaned_Content'].apply(lambda x: word_tokenize(str(x)))

# Save the updated DataFrame to a new CSV file
df.to_csv(output_csv, index=False)

print(f"Tokenized dataset saved to {output_csv}")

df

"""Removing stopwords"""

import pandas as pd
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk

# Download the stopwords and Punkt tokenizer models (run once)
nltk.download('stopwords')
nltk.download('punkt_tab')

# Input and output file paths
input_csv = 'tokenized_cleaned_tweet_emotions.csv'   # Replace with your input CSV file
output_csv = 'stopwords_removed_tokenized_cleaned_tweet_emotions.csv'  # Replace with your desired output CSV file

# Load the dataset
df = pd.read_csv(input_csv)

# Check if the column exists (assuming the column is named 'content')
if 'Cleaned_Content' not in df.columns:
    raise ValueError("The CSV file must have a 'Cleaned_Content' column with text data to clean.")

# Define stopwords
stop_words = set(stopwords.words('english'))

# Function to remove stopwords
def remove_stopwords(text):
    words = word_tokenize(str(text))  # Tokenize the text
    filtered_words = [word for word in words if word.lower() not in stop_words]
    return ' '.join(filtered_words)

# Apply the function to the 'content' column
df['No_Stopwords_Content'] = df['Cleaned_Content'].apply(remove_stopwords)

# Save the updated DataFrame to a new CSV file
df.to_csv(output_csv, index=False)

print(f"Dataset with stopwords removed saved to {output_csv}")

df

"""Stemming"""

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import nltk

# Download the Punkt tokenizer models (run once)
nltk.download('punkt_tab')

# Input and output file paths
input_csv = 'stopwords_removed_tokenized_cleaned_tweet_emotions.csv'   # Replace with your input CSV file
output_csv = 'stemmed_stopwords_removed_tokenized_cleaned_tweet_emotions.csv'  # Replace with your desired output CSV file

# Load the dataset
df = pd.read_csv(input_csv)

# Check if the column exists (assuming the column is named 'content')
if 'No_Stopwords_Content' not in df.columns:
    raise ValueError("The CSV file must have a 'No_Stopwords_Content' column with text data to stem.")

# Initialize the stemmer
stemmer = PorterStemmer()

# Function to perform stemming
def stem_text(text):
    words = word_tokenize(str(text))  # Tokenize the text
    stemmed_words = [stemmer.stem(word) for word in words]  # Stem each word
    return ' '.join(stemmed_words)

# Apply the function to the 'content' column
df['Stemmed_Content'] = df['No_Stopwords_Content'].apply(stem_text)

# Save the updated DataFrame to a new CSV file
df.to_csv(output_csv, index=False)

print(f"Dataset with stemming applied saved to {output_csv}")

df

"""Lemmatization"""

import pandas as pd
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import nltk

# Download necessary NLTK data (run once)
nltk.download('punkt_tab')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Input and output file paths
input_csv = 'stemmed_stopwords_removed_tokenized_cleaned_tweet_emotions.csv'   # Replace with your input CSV file
output_csv = 'lemmatized_stemmed_stopwords_removed_tokenized_cleaned_tweet_emotions.csv'  # Replace with your desired output CSV file

# Load the dataset
df = pd.read_csv(input_csv)

# Check if the column exists (assuming the column is named 'content')
if 'Stemmed_Content' not in df.columns:
    raise ValueError("The CSV file must have a 'Stemmed_Content' column with text data to lemmatize.")

# Initialize the lemmatizer
lemmatizer = WordNetLemmatizer()

# Function to perform lemmatization
def lemmatize_text(text):
    words = word_tokenize(str(text))  # Tokenize the text
    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize each word
    return ' '.join(lemmatized_words)

# Apply the function to the 'content' column
df['Lemmatized_Content'] = df['Stemmed_Content'].apply(lemmatize_text)

# Save the updated DataFrame to a new CSV file
df.to_csv(output_csv, index=False)

print(f"Dataset with lemmatization applied saved to {output_csv}")

df

