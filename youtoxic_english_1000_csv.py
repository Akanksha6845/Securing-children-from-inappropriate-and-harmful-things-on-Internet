# -*- coding: utf-8 -*-
"""youtoxic_english_1000.csv.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z5aVXmgLZo_U5HmfORi8RCxpSs6PvH-B
"""

import pandas as pd

from google.colab import files
uploaded = files.upload()  # A file selection dialog will appear

data=pd.read_csv('youtoxic_english_1000.csv')

data.head(10)

"""Removing punctuations and symbol"""

print(df.columns)

import pandas as pd

# Define the set of punctuations to remove
punctuations = '''!()-[]{};:'"\,<>./?@#$%^&*_~'''

# Input and output file paths
input_csv = 'youtoxic_english_1000.csv'   # Replace with your input CSV file
output_csv = 'cleaned_youtoxic_english_1000.csv'  # Replace with your desired output CSV file

# Load the dataset
df = pd.read_csv(input_csv)

# Check if the 'Text' column exists
if 'Text' not in df.columns:
    raise ValueError("The CSV file must have a 'Text' column with text data to clean.")

# Function to remove punctuation
def remove_punctuation(text):
    return ''.join(char for char in str(text) if char not in punctuations)

# Apply the function to the 'Text' column
df['Cleaned_Text'] = df['Text'].apply(remove_punctuation)

# Save the updated DataFrame to a new CSV file
df.to_csv(output_csv, index=False)

print(f"Cleaned dataset saved to {output_csv}")

df

"""Tokenization"""

import nltk
nltk.download('punkt_tab')
nltk.download('wordnet')

import pandas as pd
from nltk.tokenize import word_tokenize
import nltk

# Download necessary NLTK data (run once)
nltk.download('punkt')

# Input and output file paths
input_csv = 'cleaned_youtoxic_english_1000.csv'  # Replace with your input CSV file
output_csv = 'tokenized_cleaned_youtoxic_english_1000.csv'  # Replace with your desired output CSV file

# Load the dataset
df = pd.read_csv(input_csv)

# Check if the 'Text' column exists
if 'Cleaned_Text' not in df.columns:
    raise ValueError("The CSV file must have a 'Cleaned_Text' column with text data to tokenize.")

# Function to tokenize text
def tokenize_text(text):
    return word_tokenize(str(text))  # Tokenizes the text into words

# Apply the function to the 'Text' column
df['Tokenized_Text'] = df['Cleaned_Text'].apply(tokenize_text)

# Save the updated DataFrame to a new CSV file
df.to_csv(output_csv, index=False)

print(f"Tokenized dataset saved to {output_csv}")

df

"""stopped words"""

import pandas as pd
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk

# Download necessary NLTK data (run once)
nltk.download('stopwords')
nltk.download('punkt_tab')

# Input and output file paths
input_csv = 'tokenized_cleaned_youtoxic_english_1000.csv'  # Replace with your input CSV file
output_csv = 'stopwords_removed_tokenized_cleaned_youtoxic_english_1000.csv'  # Replace with your desired output CSV file

# Load the dataset
df = pd.read_csv(input_csv)

# Check if the 'Text' column exists
if 'Cleaned_Text' not in df.columns:
    raise ValueError("The CSV file must have a 'Cleaned_Text' column with text data to process.")

# Load stopwords
stop_words = set(stopwords.words('english'))

# Function to remove stopwords
def remove_stopwords(text):
    words = word_tokenize(str(text))  # Tokenize the text into words
    filtered_words = [word for word in words if word.lower() not in stop_words]  # Remove stopwords
    return ' '.join(filtered_words)

# Apply the function to the 'Text' column
df['Stopwords_Removed_Text'] = df['Cleaned_Text'].apply(remove_stopwords)

# Save the updated DataFrame to a new CSV file
df.to_csv(output_csv, index=False)

print(f"Dataset with stopwords removed saved to {output_csv}")

df

"""Stemming"""

import pandas as pd
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import nltk

# Download necessary NLTK data (run once)
nltk.download('punkt')

# Input and output file paths
input_csv = 'stopwords_removed_tokenized_cleaned_youtoxic_english_1000.csv'  # Replace with your input CSV file
output_csv = 'stemmed_stopwords_removed_tokenized_cleaned_youtoxic_english_1000.csv'  # Replace with your desired output CSV file

# Load the dataset
df = pd.read_csv(input_csv)

# Check if the 'Text' column exists
if 'Stopwords_Removed_Text' not in df.columns:
    raise ValueError("The CSV file must have a 'Stopwords_Removed_Text' column with text data to process.")

# Initialize the PorterStemmer
stemmer = PorterStemmer()

# Function to apply stemming
def stem_text(text):
    words = word_tokenize(str(text))  # Tokenize the text into words
    stemmed_words = [stemmer.stem(word) for word in words]  # Apply stemming
    return ' '.join(stemmed_words)

# Apply the function to the 'Text' column
df['Stemmed_Text'] = df['Stopwords_Removed_Text'].apply(stem_text)

# Save the updated DataFrame to a new CSV file
df.to_csv(output_csv, index=False)

print(f"Dataset with stemmed text saved to {output_csv}")

df

"""Lemmatization"""

import pandas as pd
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import nltk

# Download necessary NLTK data (run once)
nltk.download('punkt_tab')
nltk.download('wordnet')

# Input and output file paths
input_csv = 'stemmed_stopwords_removed_tokenized_cleaned_youtoxic_english_1000.csv'  # Replace with your input CSV file
output_csv = 'lemmatized_stemmed_stopwords_removed_tokenized_cleaned_youtoxic_english_1000.csv' # Replace with your desired output CSV file

# Load the dataset
df = pd.read_csv(input_csv)

# Check if the 'Text' column exists
if 'Stemmed_Text' not in df.columns:
    raise ValueError("The CSV file must have a 'Stemmed_Text' column with text data to process.")

# Initialize the WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

# Function to apply lemmatization
def lemmatize_text(text):
    words = word_tokenize(str(text))  # Tokenize the text into words
    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]  # Apply lemmatization
    return ' '.join(lemmatized_words)

# Apply the function to the 'Text' column
df['Lemmatized_Text'] = df['Stemmed_Text'].apply(lemmatize_text)

# Save the updated DataFrame to a new CSV file
df.to_csv(output_csv, index=False)

print(f"Dataset with lemmatized text saved to {output_csv}")

df

