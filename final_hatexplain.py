# -*- coding: utf-8 -*-
"""final_hateXplain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L91Ix83FrK8jNUmnyZvhFWU8onRO0bS1
"""

import pandas as pd

from google.colab import files
uploaded = files.upload()  # A file selection dialog will appear

data=pd.read_csv('final_hateXplain.csv')

data.head(10)

"""Removing punctuations and symbol"""

print(data.columns)

import pandas as pd

# Define the set of punctuations to remove
punctuations = '''!()-[]{};:'"\,<>./?@#$%^&*_~'''

# Input and output file paths
input_csv = 'final_hateXplain.csv'   # Replace with your input CSV file
output_csv = 'cleaned_final_hateXplain.csv'  # Replace with your desired output CSV file

# Load the dataset
df = pd.read_csv(input_csv)

# Check if the 'comment' column exists
if 'comment' not in df.columns:
    raise ValueError("The CSV file must have a 'comment' column with text data to clean.")

# Function to remove punctuation
def remove_punctuation(text):
    return ''.join(char for char in str(text) if char not in punctuations)

# Apply the function to the 'comment' column
df['Cleaned_Comment'] = df['comment'].apply(remove_punctuation)

# Save the updated DataFrame to a new CSV file
df.to_csv(output_csv, index=False)

print(f"Cleaned dataset saved to {output_csv}")

df

"""Tokenization"""

import nltk
nltk.download('punkt_tab')
nltk.download('wordnet')

import pandas as pd
from nltk.tokenize import word_tokenize
import nltk

# Download necessary NLTK data (run once)
nltk.download('punkt')

# Input and output file paths
input_csv = 'cleaned_final_hateXplain.csv'   # Replace with your input CSV file
output_csv = 'tokenized_cleaned_final_hateXplain.csv'  # Replace with your desired output CSV file

# Load the dataset
df = pd.read_csv(input_csv)

# Check if the 'comment' column exists
if 'comment' not in df.columns:
    raise ValueError("The CSV file must have a 'comment' column with text data to tokenize.")

# Function to tokenize text
def tokenize_text(text):
    return word_tokenize(str(text))

# Apply the function to the 'comment' column
df['Tokenized_Comment'] = df['comment'].apply(tokenize_text)

# Save the updated DataFrame to a new CSV file
df.to_csv(output_csv, index=False)

print(f"Tokenized dataset saved to {output_csv}")

df

"""stopped words"""

import pandas as pd
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk

# Download necessary NLTK data (run once)
nltk.download('punkt')
nltk.download('stopwords')

# Input and output file paths
input_csv = 'tokenized_cleaned_final_hateXplain.csv'   # Replace with your input CSV file
output_csv = 'no_stopwords_tokenized_cleaned_final_hateXplain.csv'  # Replace with your desired output CSV file

# Load the dataset
df = pd.read_csv(input_csv)

# Check if the 'comment' column exists
if 'comment' not in df.columns:
    raise ValueError("The CSV file must have a 'comment' column with text data to process.")

# Get the list of English stopwords
stop_words = set(stopwords.words('english'))

# Function to remove stopwords
def remove_stopwords(text):
    words = word_tokenize(str(text))  # Tokenize the text
    filtered_words = [word for word in words if word.lower() not in stop_words]  # Remove stopwords
    return ' '.join(filtered_words)

# Apply the function to the 'comment' column
df['No_Stopwords_Comment'] = df['comment'].apply(remove_stopwords)

# Save the updated DataFrame to a new CSV file
df.to_csv(output_csv, index=False)

print(f"Dataset with stopwords removed saved to {output_csv}")

df

"""Stemming"""

import pandas as pd
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import nltk

# Download necessary NLTK data (run once)
nltk.download('punkt')

# Input and output file paths
input_csv = 'no_stopwords_tokenized_cleaned_final_hateXplain.csv'   # Replace with your input CSV file
output_csv = 'stemmed_no_stopwords_tokenized_cleaned_final_hateXplain.csv'  # Replace with your desired output CSV file

# Load the dataset
df = pd.read_csv(input_csv)

# Check if the 'comment' column exists
if 'comment' not in df.columns:
    raise ValueError("The CSV file must have a 'comment' column with text data to process.")

# Initialize the stemmer
stemmer = PorterStemmer()

# Function to perform stemming
def stem_text(text):
    words = word_tokenize(str(text))  # Tokenize the text
    stemmed_words = [stemmer.stem(word) for word in words]  # Stem each word
    return ' '.join(stemmed_words)

# Apply the function to the 'comment' column
df['Stemmed_Comment'] = df['comment'].apply(stem_text)

# Save the updated DataFrame to a new CSV file
df.to_csv(output_csv, index=False)

print(f"Dataset with stemming applied saved to {output_csv}")

df

"""Lemmatization"""

import pandas as pd
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import nltk

# Download necessary NLTK data (run once)
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Input and output file paths
input_csv = 'stemmed_no_stopwords_tokenized_cleaned_final_hateXplain.csv'   # Replace with your input CSV file
output_csv = 'lemmatized_stemmed_no_stopwords_tokenized_cleaned_final_hateXplain.csv'  # Replace with your desired output CSV file

# Load the dataset
df = pd.read_csv(input_csv)

# Check if the 'comment' column exists
if 'comment' not in df.columns:
    raise ValueError("The CSV file must have a 'comment' column with text data to process.")

# Initialize the lemmatizer
lemmatizer = WordNetLemmatizer()

# Function to perform lemmatization
def lemmatize_text(text):
    words = word_tokenize(str(text))  # Tokenize the text
    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize each word
    return ' '.join(lemmatized_words)

# Apply the function to the 'comment' column
df['Lemmatized_Comment'] = df['comment'].apply(lemmatize_text)

# Save the updated DataFrame to a new CSV file
df.to_csv(output_csv, index=False)

print(f"Dataset with lemmatization applied saved to {output_csv}")

df

